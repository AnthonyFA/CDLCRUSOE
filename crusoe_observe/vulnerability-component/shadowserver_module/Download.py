#!/usr/bin/env python

"""
Module responsible for downloading files from the Shadowserver.
"""

import requests
import sys
import cgi
import structlog
import certifi

from urllib.request import urlopen
from urllib.error import URLError
from bs4 import BeautifulSoup
from shadowserver_module.Remove import remove_corrupted_files

def parse_current_day(shadowserver_html):
    """
    Parse newest links from shadowserver web page.(newest day)
    :return: Parsed data of current day
    """

    current_year = shadowserver_html.li
    for year in shadowserver_html.li.next_siblings:
        if year != '\n':
            current_year = year

    current_month = current_year.li
    for month in current_year.li.next_siblings:
        if month != '\n':
            current_month = month

    current_day = current_month.li
    for day in current_month.li.next_siblings:
        if day != '\n':
            current_day = day

    return current_day

def current_day_scan(location, user, password, logger=structlog.get_logger()):
    """
    Connect to shadowserver, parse most actual files and download them to the current directory.
    :param location: where will be downloaded csv saved
    :param user: username login credential
    :param password: password for username specified in previous argument
    :param logger: logger for the method
    :return: True if task was successful, exit with error otherwise
    """
    payload = {
        'user': user,
        'password': password,
    }
    logger.info("Signing into Shadowserver...")
    try:
        response = requests.post('https://dl.shadowserver.org/reports/index.php', data=payload)
        if "Invalid username/password combination" in response.text:
            raise requests.HTTPError("Invalid credentials")

        logger.info("Getting list of files which will be downloaded...")

        soup = BeautifulSoup(response.text, 'xml')
        number_of_downloaded_files = 0

        for section in parse_current_day(soup).find_all('a', href=True):
            url = section['href']
            try:
                remote_file = urlopen(url, cafile=certifi.where()) #nosec, url from trusted source(shadowserver)
                content = remote_file.info()['Content-Disposition']
                _, params = cgi.parse_header(content)
                filename = location + params["filename"]
                file = open(filename, 'wb')
                file.write(remote_file.read())
                file.close()
                logger.info(f'File {params["filename"]} has been downloaded')
                number_of_downloaded_files += 1
            except URLError as error: # Catch exception linked to urllib file retrieving add entry to log and skip them
                logger.warning(f'File {params["filename"]} has\'t been downloaded and won\'t be processed',
                               error_message=error)

        logger.info("Files downloaded: %s" % number_of_downloaded_files)

        remove_corrupted_files(location)
    except requests.RequestException as error: # Catch exceptions raised by request library and exit
        logger.error("Can't connect and sign into Shadowserver", error_message=error)
        sys.exit(1)
    return True
